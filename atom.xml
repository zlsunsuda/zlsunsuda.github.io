<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sun Zhu Li&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/5cb85443a2dec50b8221e9f83b67e95b</icon>
  <subtitle>Hello World</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zlsunsuda.github.io/"/>
  <updated>2018-10-03T02:55:34.251Z</updated>
  <id>https://zlsunsuda.github.io/</id>
  
  <author>
    <name>Sun Zhu Li</name>
    <email>learn_sunzhuli@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Elasticsearch issue</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/Elasticsearch%20issue/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/Elasticsearch issue/</id>
    <published>2018-10-03T02:52:20.000Z</published>
    <updated>2018-10-03T02:55:34.251Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Elasticsearch-日志中报错“Too-many-open-files”"><a href="#Elasticsearch-日志中报错“Too-many-open-files”" class="headerlink" title="Elasticsearch 日志中报错“Too many open files”"></a>Elasticsearch 日志中报错“Too many open files”</h2><p>检查系统允许 Elasticsearch 打开的最大文件数。<br>(1) 检查linux文件句柄，默认为1024<br>ulimit -a<br>open files  (-n) 1024</p><p>(2) 修改配置文件<br>        vi /etc/security/limits.conf</p><pre><code>* soft nofile 65535* hard nofile 65535root soft nofile 65535root hard nofile 65535</code></pre><p>(3) reboot Elasticsearch所在虚拟机</p><h2 id="如果虚拟机内存分配不足，会导致es启动失败"><a href="#如果虚拟机内存分配不足，会导致es启动失败" class="headerlink" title="如果虚拟机内存分配不足，会导致es启动失败"></a>如果虚拟机内存分配不足，会导致es启动失败</h2><p>要求：heap size [9.9gb]，一般分配12G内存</p><pre><code>[2018-01-19 14:58:12,732][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are neededKilled[2018-01-19 15:52:59,321][INFO ][env                      ] [sa-node-1] heap size [9.9gb], compressed ordinary object pointers [true]</code></pre><p>因为在配置文件中设置了，es启动后需要一次性获取足够内存。<br>cat /opt/elasticsearch-2.4.3/config/elasticsearch.yml<br>bootstrap.memory_lock: true</p><p>也可以设置成不一次性获取所有内存：<br>bootstrap.memory_lock: false<br>bootstrap.system_call_filter: false  （该配置必须位于上一条配置之后）</p><p>重启Elasticsearch后异常消失</p><h2 id="Elasticsearch-JVM优化"><a href="#Elasticsearch-JVM优化" class="headerlink" title="Elasticsearch JVM优化"></a>Elasticsearch JVM优化</h2><p>可以根据机器硬件配置情况作出适当的调整ES的JVM内存配置，一般情况下，此处的内存分配大小为机器物理内存的一半，同时将 ES_MIN_MEM 与 ES_MAX_MEM 配置成相同的值，这样的好处在于ES JVM大小固定，不会上下浮动，从实践效果上看可以提高 node 性能。<br>vim  /opt/elasticsearch-2.4.3/elasticsearch.in.sh</p><h2 id="Elasticsearch-cache-异常-Data-too-large"><a href="#Elasticsearch-cache-异常-Data-too-large" class="headerlink" title="Elasticsearch cache 异常( Data too large)"></a>Elasticsearch cache 异常( Data too large)</h2><p>org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed<br>        at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206)<br>        at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:152)<br>        at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)<br>        at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:874)<br>        at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:852)<br>        at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:389)<br>        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)<br>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)<br>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)<br>        at java.lang.Thread.run(Thread.java:745)<br>Caused by: org.elasticsearch.ElasticsearchException: ElasticsearchException[CircuitBreakingException[[fielddata] Data too large, data for [zone] would be larger than limit of [6390113894/5.9gb]]]; nested: UncheckedExecutionException[CircuitBreakingException[[fielddata] Data too large, data for [zone] would be larger than limit of [6390113894/5.9gb]]]; nested: CircuitBreakingException[[fielddata] Data too large, data for [zone] would be larger than limit of [6390113894/5.9gb]];<br>        at org.elasticsearch.index.fielddata.plain.AbstractIndexOrdinalsFieldData.loadGlobal(AbstractIndexOrdinalsFieldData.java:91)<br>        at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$WithOrdinals$FieldData.globalOrdinalsValues(ValuesSource.java:144)<br>        at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$WithOrdinals.globalMaxOrd(ValuesSource.java:117)<br>        at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.doCreateInternal(TermsAggregatorFactory.java:217)<br>        at org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.createInternal(ValuesSourceAggregatorFactory.java:64)<br>        at org.elasticsearch.search.aggregations.AggregatorFactory.create(AggregatorFactory.java:102)<br>        at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:87)<br>```</p><p>原因：由于cache溢出导致<br>在linux中查看cache大小?<br>free -hl<br>buffers(buffer cache), 块设备的缓冲区，协调memory与disk.<br>cache(page cache), 用于给文件做缓存,记录打开过的文件。协调CPU与memory.</p><p>solution:<br>(1) 手动删除cache:<br>curl -XPOST  ‘192.168.122.5:9200/_cache/clear’</p><p>(2)修改es配置文件<br>indices.fielddata.cache.size: 20%<br>当cache达到20%，则自动清理缓存。<br>indices.fielddata.cache.size默认值为无限大，则不会自动回收缓存。会抛出Data too large异常。</p><ol><li>index异常处理<br>当索引不存在或者不可用时，不抛出异常。默认值都为false。<br>allow_no_indices: true<br>ignore_unavailable: true</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Elasticsearch-日志中报错“Too-many-open-files”&quot;&gt;&lt;a href=&quot;#Elasticsearch-日志中报错“Too-many-open-files”&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearc
      
    
    </summary>
    
      <category term="Elasticsearch" scheme="https://zlsunsuda.github.io/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="https://zlsunsuda.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Java 数据绑定</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/Java%20%E6%95%B0%E6%8D%AE%E7%BB%91%E5%AE%9A/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/Java 数据绑定/</id>
    <published>2018-10-03T02:52:20.000Z</published>
    <updated>2018-10-03T02:56:02.567Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据绑定问题"><a href="#数据绑定问题" class="headerlink" title="数据绑定问题"></a>数据绑定问题</h2><p>(1) 方法首先进入getParamBean方法，然后进入index方法。<br>(2) 如果进入getParamBean方法，user，logType为中文（注意在html中提交时name, value取值），则在getParamBean中绑定AuditLogSearchBean在index方法中不生效，会发生二次绑定。<br>@ModelAttribute<br>public AuditLogSearchBean getParamBean(String mStartDate, String mEndDate, String searchText, String user,<br>        String logType) {<br>    // TODO<br>}</p><p>@RequestMapping(value = { “”, “/list”, “/index” })<br>public String index(AuditLogSearchBean searchParamBean, HttpServletRequest request, Model model) {<br>    // TODO<br>}</p><h2 id="在-ModelAttribute中绑定的数据可能会被覆盖"><a href="#在-ModelAttribute中绑定的数据可能会被覆盖" class="headerlink" title="在@ModelAttribute中绑定的数据可能会被覆盖"></a>在@ModelAttribute中绑定的数据可能会被覆盖</h2><p>@ModelAttribute<br>public LogDataSource getLogDataSource(@RequestParam(required = false) String id,@RequestParam(required = false) String[] appName, @RequestParam(required = false) String[] appType,@RequestParam(required = false) String[] appPath) {<br>    // TODO<br>}</p><h2 id="aop"><a href="#aop" class="headerlink" title="aop"></a>aop</h2><p>AspectJ AOP<br>Spring AOP<br>JDK proxy和CGlib</p><p>@Before是在所拦截方法执行之前执行一段逻辑。<br>@After 是在所拦截方法执行之后执行一段逻辑。<br>@Around是可以同时在所拦截方法的前后执行一段逻辑。</p><p>切面（Aspect）：官方的抽象定义为“一个关注点的模块化，这个关注点可能会横切多个对象”，在本例中，“切面”就是类TestAspect所关注的具体行为，例如，AServiceImpl.barA()的调用就是切面TestAspect所关注的行为之一。“切面”在ApplicationContext中<aop:aspect>来配置。</aop:aspect></p><p>连接点（Joinpoint） ：程序执行过程中的某一行为，例如，UserService.get的调用或者UserService.delete抛出异常等行为。</p><p>通知（Advice） ：“切面”对于某个“连接点”所产生的动作，例如，TestAspect中对com.spring.service包下所有类的方法进行日志记录的动作就是一个Advice。其中，一个“切面”可以包含多个“Advice”，例如ServiceAspect。</p><p>切入点（Pointcut） ：匹配连接点的断言，在AOP中通知和一个切入点表达式关联。例如，TestAspect中的所有通知所关注的连接点，都由切入点表达式execution(<em> com.spring.service.</em>.*(..))来决定。</p><p>目标对象（Target Object） ：被一个或者多个切面所通知的对象。例如，AServcieImpl和BServiceImpl，当然在实际运行时，Spring AOP采用代理实现，实际AOP操作的是TargetObject的代理对象。</p><p>AOP代理（AOP Proxy） ：在Spring AOP中有两种代理方式，JDK动态代理和CGLIB代理。默认情况下，TargetObject实现了接口时，则采用JDK动态代理，例如，AServiceImpl；反之，采用CGLIB代理，例如，BServiceImpl。强制使用CGLIB代理需要将 aop:config的 proxy-target-class属性设为true。</p><h2 id="stream"><a href="#stream" class="headerlink" title="stream"></a>stream</h2><p>collect方法和reduce方法不同，reduce方法在迭代时，每次都会生成新的对象（值）（通常情况下，reduce的中间对象是int之类的东西，每次计算，都生成一个新的，而不是改变老的。），但是collect方法在迭代时是改变中间生成的对象（通常情况下，collect的中间对象是诸如List之类的东西，可以把元素加进去。）</p><p>我们看到collect是一个将管道流的结果集到一个list中的结束操作。collect是一个将数据流缩减为一个值的归约操作。这个值可以是集合、映射，或者一个值对象。你可以使用collect达到以下目的：<br>将数据流缩减为一个单一值：<br>将一个数据流中的元素进行分组：</p><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>Synchronized用于线程间的数据共享，而ThreadLocal则用于线程间的数据隔离。所以ThreadLocal的应用场合，最适合的是按线程多实例（每个线程对应一个实例）的对象的访问，并且这个对象很多地方都要用到。<br>java 线程池<br>需要解决的问题：<br>（1）主线程等待子线程执行<br>（2）获取线程执行过程中抛出的异常</p><p>ExecutorService<br>invokeAll，为同步方法，会阻塞, 等待所有任务执行后返回<br>submit，为异步方法，不会阻塞，有返回值，能捕捉异常<br>excute方法，没有返回值，不能捕捉异常<br>ExecutorService配合Callable使用<br>FutureTask<br>get()获取执行结果，接收是否有异常抛出。get 方法会阻塞<br>CompletableFuture</p><h2 id="可变参数列表"><a href="#可变参数列表" class="headerlink" title="可变参数列表"></a>可变参数列表</h2><p>public void foo(String…varargs){}<br>foo(“arg1”, “arg2”, “arg3”);<br>//上述过程和下面的调用是等价的<br>foo(new String[]{“arg1”, “arg2”, “arg3”});</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据绑定问题&quot;&gt;&lt;a href=&quot;#数据绑定问题&quot; class=&quot;headerlink&quot; title=&quot;数据绑定问题&quot;&gt;&lt;/a&gt;数据绑定问题&lt;/h2&gt;&lt;p&gt;(1) 方法首先进入getParamBean方法，然后进入index方法。&lt;br&gt;(2) 如果进入getPa
      
    
    </summary>
    
      <category term="Java" scheme="https://zlsunsuda.github.io/categories/Java/"/>
    
    
      <category term="Java" scheme="https://zlsunsuda.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Linux iptables与监控工具</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/Linux%20iptables%E4%B8%8E%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/Linux iptables与监控工具/</id>
    <published>2018-10-03T02:52:20.000Z</published>
    <updated>2018-10-03T02:58:15.165Z</updated>
    
    <content type="html"><![CDATA[<h2 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h2><ol><li><p>临时性生效<br>service iptables status<br>service iptables save<br>iptables -A INTPUT -p tcp –sport 27017 -j ACCEPT<br>iptables -D INPUT number</p></li><li><p>修改配置文件（永久性生效）<br>可以增加/删除端口<br>cat /etc/sysconfig/iptables<br>vi /etc/sysconfig/iptables</p></li></ol><p>-A INPUT -m state –state NEW -m tcp -p tcp –dport 27017 -j ACCEPT<br>注意：（1）-m state –state为一个完整参数<br>（2）必须注意顺序。<br>被允许的端口必须放在REJECT之前。<br>（3）不需要增加-A OUTPUT 规则。<br>重启iptables则生效： service iptables restart<br>-A INPUT -p tcp -m state –state NEW -m tcp –dport 27017 -j ACCEPT<br>-A INPUT -j REJECT –reject-with icmp-host-prohibited<br>-A FORWARD -j REJECT –reject-with icmp-host-prohibited</p><p>service iptables status查看端口结果<br>5    ACCEPT     tcp  –  0.0.0.0/0            0.0.0.0/0           state NEW tcp dpt:27017<br>6    REJECT     all  –  0.0.0.0/0            0.0.0.0/0           reject-with icmp-host-prohibited</p><ol><li><p>es<br>需要开放两个port:<br>-A INPUT -p tcp -m state –state NEW -m tcp –dport 9200 -j ACCEPT<br>-A INPUT -p tcp -m state –state NEW -m tcp –dport 9300 -j ACCEPT</p></li><li><p>查看现有规则链中详情<br>iptables -L </p></li></ol><h2 id="linux-系统监测工具"><a href="#linux-系统监测工具" class="headerlink" title="linux 系统监测工具"></a>linux 系统监测工具</h2><ol><li><p>iostat，通过yum安装<br>iostat -d -x 2<br>观察avctm, await 两个参数</p></li><li><p>netstat -ant<br>查看tcp网络连接</p><p>netstat -antu<br>查看tcp/udp网络连接</p><p>netstat -antup<br>查看tcp/udp网络连接，以及进程名字</p></li></ol><p>-p 查看进程名<br>-n 能显示为数字的全部转成数字</p><ol><li><p>查看系统的负载<br>vmstat -S m 2<br>-S: 使用指定单位显示</p></li><li><p>查看网络通信，用wireshark更强大<br>tcpdump -i eth0<br>eth0 为第一个网卡, lo 为localhost。通过ifconfig查看</p></li><li><p>查看内存和CPU<br>top<br>等同于： uptime, free, mpstat<br>先top，然后按1，切换到多核模式，查看各个CPU情况</p></li></ol><p>shift + M 按内存大小排序</p><p>mpstat 可以查看Linux版本与CPU个数</p><p>%MEM 进程使用的物理内存百分比<br>RES 进程使用的物理内存<br>VIRT 进程使用的物理内存</p><ol><li>telnet 可以探测端口是否开放<br>telnet ip port<br>e.g. telnet 10.11.42.151 6379<br>退出telnet： ctrl+]，然后按q</li></ol><p>ping 探测主机是否存活</p><ol><li>iptables，配置防火墙规则<br>出口表<br>入口表<br>iptables -A OUTPUT -p tcp –dport 6379 -j DROP<br>iptables -D OUTPUT -p tcp –dport 6379 -j DROP</li></ol><p>-p 协议<br>–dport 端口<br>-j 动作，指定如何处理<br>-D 删除规则<br>-A 在当前链的最后新增一个规则</p><ol><li><p>ps<br>查看pid为10696中线程情况<br>tid为线程号<br>time为占用cpu时间<br>注意：THREAD,tid,time间不能有空格<br>ps -mp 10696 -o THREAD,tid,time<br>ps -mp 10696 -o THREAD,tid,time,rss,size,%mem<br>ps aux<br>ps -ef</p></li><li><p>打印java堆栈信息<br>29ca为j将tid转换成16进制的结果<br>jstack 10696 | grep 29ca -A 30</p></li><li><p>查看线程的内存占用<br>分析具体的对象数目和占用内存的大小<br>jmap -histo:live 10696 | head -n 100</p></li><li><p>查看进程中各个线程CPU占用<br>10696 为pid<br>top -p 10696 -H</p></li><li><p>查看java进程<br>jps<br>top -p 10696 -H<br>jstack 10696 | grep 29ca -A 30</p></li><li><p>高CPU占用问题排查<br>jps<br>查看线程占用cpu时间（有两个命令）<br>ps -mp 10696 -o THREAD,tid,time<br>top -p 10696 -H<br>jstack 10696 | grep 29ca -A 30</p></li><li><p>高内存问题排产<br>jmap -histo:live 10696 | head -n 100</p></li><li><p>查找应用程序所在位置<br>which jvm<br>whereis搜索更大范围<br>whereis jvm</p></li><li><p>Linux中采取监控数据<br>dstat<br>ntop<br>snmp 简单网络管理协议</p></li><li><p>linux下软件安装与卸载命令<br>rpm: redhat package manager<br>rpm -qa | grep iotop</p></li><li><p>查看磁盘读写速度<br>iostop -o </p></li><li><p>输出重定向<br>iotop -o &gt;&gt;bi_disk.txt</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;iptables&quot;&gt;&lt;a href=&quot;#iptables&quot; class=&quot;headerlink&quot; title=&quot;iptables&quot;&gt;&lt;/a&gt;iptables&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;临时性生效&lt;br&gt;service iptables status&lt;br&gt;s
      
    
    </summary>
    
      <category term="Linux" scheme="https://zlsunsuda.github.io/categories/Linux/"/>
    
    
      <category term="iptables" scheme="https://zlsunsuda.github.io/tags/iptables/"/>
    
  </entry>
  
  <entry>
    <title>Python on centos</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/Python%20on%20centos/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/Python on centos/</id>
    <published>2018-10-03T02:52:20.000Z</published>
    <updated>2018-10-03T03:00:49.948Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装依赖库"><a href="#安装依赖库" class="headerlink" title="安装依赖库"></a>安装依赖库</h2><p>python 2.6.6安装pip<br>yum install -y python-pip<br>或者手动安装pip, setuptools</p><h2 id="python升级"><a href="#python升级" class="headerlink" title="python升级"></a>python升级</h2><p>python2.6升级到python3.6<br>先确保yum可用，且在root用户下运行 update_to_python3.sh脚本<br>python2.6升级到python2.7，需要安装pip, setuptools。而python3.6已经自带了。<br>（1）下载压缩包Python-3.6.1.tgz<br>（2）安装gcc<br>yum install gcc<br>（3）安装zlib<br>yum -y install zlib*<br>（4）修改yum源（手动去执行）<br>vi /usr/bin/yum<br>将#!/usr/bin/python修改为#!/usr/bin/python2.6<br>（5）设置pip源（先需要手动创建目录）（该步骤手动去执行）<br> mkdir ~/.pip<br> vi ~/.pip/pip.conf<br> 通过重定向写入：<br>[global]<br>timeout=60<br>index=<a href="http://10.11.105.11:8081/repository/python/pypi" target="_blank" rel="external">http://10.11.105.11:8081/repository/python/pypi</a><br>index-url=<a href="http://10.11.105.11:8081/repository/python/simple" target="_blank" rel="external">http://10.11.105.11:8081/repository/python/simple</a></p><p>[install]<br>trusted-host=10.11.105.11</p><p>(6) 安装pymongo（该步骤手动去执行，需要先执行步骤五）<br> pip3 install pymongo</p><h2 id="python2-6-升级到python2-7"><a href="#python2-6-升级到python2-7" class="headerlink" title="python2.6 升级到python2.7"></a>python2.6 升级到python2.7</h2><p>在root用户下运行 update_to_python2.sh脚本<br>如下步骤需要手动去执行：<br>修改yum源<br>设置pip源<br>安装pymongo</p><h2 id="python-package相对导入，绝对导入"><a href="#python-package相对导入，绝对导入" class="headerlink" title="python package相对导入，绝对导入"></a>python package相对导入，绝对导入</h2><p>(1) python工程运行在windows环境，借助pycharm IDE, 能自动组织导入<br>(2) 运行在centos中，很容易出现模块找不到情况。<br>solution:<br>glance为顶层包，包括operation, util两个子包。glance包内全部采用相对导入。<br>程序主入口mian.py要放在与glance同一级目录。mian.py中只能采用绝对导入。<br>则不要再次加入，sys.path.append(os.path.abspath(‘../../glance’))</p><h2 id="python3-报错ssl"><a href="#python3-报错ssl" class="headerlink" title="python3 报错ssl"></a>python3 报错ssl</h2><p>solution: 参考update_to_python3_with_ssl.sh<br>在编译之前增加：<br>yum -y install openssl-devel</p><p>./configure –prefix=/usr/local/python3/ –with-ssl</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;安装依赖库&quot;&gt;&lt;a href=&quot;#安装依赖库&quot; class=&quot;headerlink&quot; title=&quot;安装依赖库&quot;&gt;&lt;/a&gt;安装依赖库&lt;/h2&gt;&lt;p&gt;python 2.6.6安装pip&lt;br&gt;yum install -y python-pip&lt;br&gt;或者手动安装pi
      
    
    </summary>
    
      <category term="Python" scheme="https://zlsunsuda.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://zlsunsuda.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Keras实践</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/keras%E5%AE%9E%E8%B7%B5/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/keras实践/</id>
    <published>2018-10-03T02:52:20.000Z</published>
    <updated>2018-10-03T02:56:44.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="初始化器的用法"><a href="#初始化器的用法" class="headerlink" title="初始化器的用法"></a>初始化器的用法</h2><p>初始化定义了设置 Keras 各层权重随机初始值的方法。<br>用来将初始化器传入 Keras 层的参数名取决于具体的层。通常关键字为 kernel_initializer 和 bias_initializer</p><pre><code># Compatibility aliaseszero = zeros = Zerosone = ones = Onesconstant = Constantuniform = random_uniform = RandomUniformnormal = random_normal = RandomNormaltruncated_normal = TruncatedNormalidentity = Identityorthogonal = Orthogonal</code></pre><h2 id="正则化器的使用"><a href="#正则化器的使用" class="headerlink" title="正则化器的使用"></a>正则化器的使用</h2><p>正则化器允许在优化过程中对层的参数或层的激活情况进行惩罚。 网络优化的损失函数也包括这些惩罚项。<br>惩罚是以层为对象进行的。具体的 API 因层而异，但 Dense，Conv1D，Conv2D 和 Conv3D 这些层具有统一的 API。<br>正则化器开放 3 个关键字参数：</p><pre><code>kernel_regularizer: keras.regularizers.Regularizer 的实例bias_regularizer: keras.regularizers.Regularizer 的实例activity_regularizer: keras.regularizers.Regularizer 的实例keras.regularizers.l1(0.)keras.regularizers.l2(0.)keras.regularizers.l1_l2(l1=0.01, l2=0.01)</code></pre><h2 id="约束项的使用"><a href="#约束项的使用" class="headerlink" title="约束项的使用"></a>约束项的使用</h2><p>constraints 模块的函数允许在优化期间对网络参数设置约束（例如非负性）。约束是以层为对象进行的。<br>约束层开放 2 个关键字参数：<br>    kernel_constraint 用于主权重矩阵。<br>    bias_constraint 用于偏置。</p><pre><code>max_norm(max_value=2, axis=0): 最大范数约束non_neg(): 非负性约束unit_norm(axis=0): 单位范数约束min_max_norm(min_value=0.0, max_value=1.0, rate=1.0, axis=0): 最小/最大范数约束</code></pre><h2 id="优化器的用法"><a href="#优化器的用法" class="headerlink" title="优化器的用法"></a>优化器的用法</h2><pre><code># Aliases.sgd = SGDrmsprop = RMSpropadagrad = Adagradadadelta = Adadeltaadam = Adamadamax = Adamaxnadam = Nadam</code></pre><h2 id="评价函数的用法"><a href="#评价函数的用法" class="headerlink" title="评价函数的用法"></a>评价函数的用法</h2><p>评价函数用于评估当前训练模型的性能。<br>binary_accuracy<br>categorical_accuracy<br>accuracy</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>categorical_crossentropy<br>binary_crossentropy<br>sparse_categorical_crossentropy<br>kullback_leibler_divergence<br>poisson<br>cosine_proximity<br>mse = MSE = mean_squared_error<br>mae = MAE = mean_absolute_error<br>mape = MAPE = mean_absolute_percentage_error<br>msle = MSLE = mean_squared_logarithmic_error<br>cosine = cosine_proximity</p><h2 id="tensor维度"><a href="#tensor维度" class="headerlink" title="tensor维度"></a>tensor维度</h2><p>第一维为12表示样本个数为12，第二维为空，表示特征维度不确定<br>(12,)<br>model.add(Reshape((3, 4), input_shape=(12,)))</p><p>还支持使用 -1 表示维度的尺寸推断<br>model.add(Reshape((-1, 2, 2)))</p><p>第一维为None表示样本个数不确定，第二维为32，表示特征维度为32<br>model.output_shape == (None, 32)</p><h2 id="issue"><a href="#issue" class="headerlink" title="issue"></a>issue</h2><p>(1) keras ValueError a dot layer should be called on a list of 2 inputs<br>(2) “The Merge layer is deprecated and will be removed after 08/2017. Useinstead layers from keras.layers.merge, e.g. add, concatenate, etc.” </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;初始化器的用法&quot;&gt;&lt;a href=&quot;#初始化器的用法&quot; class=&quot;headerlink&quot; title=&quot;初始化器的用法&quot;&gt;&lt;/a&gt;初始化器的用法&lt;/h2&gt;&lt;p&gt;初始化定义了设置 Keras 各层权重随机初始值的方法。&lt;br&gt;用来将初始化器传入 Keras 层的
      
    
    </summary>
    
      <category term="Keras" scheme="https://zlsunsuda.github.io/categories/Keras/"/>
    
    
      <category term="Keras" scheme="https://zlsunsuda.github.io/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>seaborn画图</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/seaborn/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/seaborn/</id>
    <published>2018-10-03T02:52:20.000Z</published>
    <updated>2018-10-03T03:01:37.601Z</updated>
    
    <content type="html"><![CDATA[<h2 id="修改图片尺寸"><a href="#修改图片尺寸" class="headerlink" title="修改图片尺寸"></a>修改图片尺寸</h2><p>from matplotlib import pyplot as plt<br>plt.figure(figsize=(100, 5))<br>通过以上两行起作用<br>sns.barplot(x=age_survived.index, y=age_survived[‘Survived’] + age_survived[‘Dead’], color=’red’)<br>sns.barplot(x=age_survived.index, y=age_survived[‘Survived’], color=’green’)</p><p>数据探索性分析</p><h2 id="区分boxplot-barplot"><a href="#区分boxplot-barplot" class="headerlink" title="区分boxplot, barplot"></a>区分boxplot, barplot</h2><p>barplot与boxplot画图效果不一致，barplot不能显示较高精度。boxplot可以显示均值范围以及异常值个数。</p><h2 id="数据离散程度以及异常值"><a href="#数据离散程度以及异常值" class="headerlink" title="数据离散程度以及异常值"></a>数据离散程度以及异常值</h2><p>sns.pairplot(iris, hue=’Name’)<br>单个变量离散程度<br>sns.scatterplot(x=train.index, y=train.SibSp)</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><p>由pandas提供boxplot方法<br>iris.boxplot(by=”Name”, figsize=(12, 6))</p><h2 id="roc-auc"><a href="#roc-auc" class="headerlink" title="roc/auc"></a>roc/auc</h2><p>y_test若不为二分类label，则需要指定pos_label, pos_labelz作为正例，其余class都作为反例<br>fpr, tpr, threshold = roc_curve(y_test, svc.decision_function(X_test)[:, 1], pos_label=1)<br>sns.lineplot(x=fpr, y=tpr)</p><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p>sns.heatmap(confusion_matrix(y_test, svc.predict(X_test)), annot=True)</p><h2 id="grid-search查找参数"><a href="#grid-search查找参数" class="headerlink" title="grid search查找参数"></a>grid search查找参数</h2><p>y轴为param_grid中第一个参数，x轴为param_grid中第二个参数，但不能设置xlable, ylabel<br>sns.heatmap(scores, yticklabels=param_grid[‘C’], xticklabels=param_grid[‘gamma’],<br>            annot=True, )</p><h2 id="特征重要性"><a href="#特征重要性" class="headerlink" title="特征重要性"></a>特征重要性</h2><p>设置orient，且交换x, y<br>sns.barplot(x=sorted_feature_importances, y=sorted_feature_names, orient=’h’)<br>sns.barplot(x=sorted_feature_names, y=sorted_feature_importances)</p><h2 id="keras-model-train-history"><a href="#keras-model-train-history" class="headerlink" title="keras model train_history"></a>keras model train_history</h2><p>构建一个DataFrame，x=’epoches’, y=’acc’分别为DataFrame key<br>sns.lineplot(x=’epoches’, y=’acc’, data=train_history_df, markers=True)</p><p>由dict构建一个DataFrame<br>history_dic = train_history.history<br>epochs = np.shape(train_history.history[‘acc’])[0]<br>history_dic[‘epoches’] = range(epochs)</p><h2 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h2><p>ratings[‘rating’]为pandas中Series结构<br>sns.distplot(ratings[‘rating’])</p><p>matploitlib直方图<br>plt.hist(ratings[‘rating’])</p><h2 id="对离散变量，且取值个数较少情况"><a href="#对离散变量，且取值个数较少情况" class="headerlink" title="对离散变量，且取值个数较少情况"></a>对离散变量，且取值个数较少情况</h2><p>用barplot或者catplot，可以引用hue属性，查看3个变量情况<br>sns.barplot(x=’SibSp’, y=’Survived’, hue=’Sex’, data=train_ready)<br>sns.catplot(x=’SibSp’, y=’Survived’, hue=’Sex’, data=train_ready, kind=’bar’)<br>sns.catplot(x=’SibSp’, y=’Survived’, hue=’Sex’, data=train_ready, kind=’box’)</p><h2 id="对连续变量，且取值个数较多情况（如Age-Fare）"><a href="#对连续变量，且取值个数较多情况（如Age-Fare）" class="headerlink" title="对连续变量，且取值个数较多情况（如Age, Fare）"></a>对连续变量，且取值个数较多情况（如Age, Fare）</h2><p>不能用barplot，否则会由于bar过多而密集</p><p>用distplot，不允许数据中有空值，需要先填充空值<br>sns.distplot(a=train_ready[‘Age’])</p><p>用FacetGrid，允许数据中有空值<br>g = sns.FacetGrid(train_ready, col=’Survived’)<br>g.map(sns.distplot, ‘Age’)</p><h1 id="柱状图叠加效果，计算总量及比率-（查找更简洁方法）"><a href="#柱状图叠加效果，计算总量及比率-（查找更简洁方法）" class="headerlink" title="柱状图叠加效果，计算总量及比率 （查找更简洁方法）"></a>柱状图叠加效果，计算总量及比率 （查找更简洁方法）</h1><p>sns.barplot(x=sex_survived.index, y=sex_survived[‘Survived’]+sex_survived[‘Dead’], color=’red’)<br>sns.barplot(x=sex_survived.index, y=sex_survived[‘Survived’], color=’green’)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;修改图片尺寸&quot;&gt;&lt;a href=&quot;#修改图片尺寸&quot; class=&quot;headerlink&quot; title=&quot;修改图片尺寸&quot;&gt;&lt;/a&gt;修改图片尺寸&lt;/h2&gt;&lt;p&gt;from matplotlib import pyplot as plt&lt;br&gt;plt.figure(fig
      
    
    </summary>
    
      <category term="seaborn" scheme="https://zlsunsuda.github.io/categories/seaborn/"/>
    
    
      <category term="seaborn" scheme="https://zlsunsuda.github.io/tags/seaborn/"/>
    
  </entry>
  
  <entry>
    <title>Spark in Java</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/spark%20java%20%E5%AE%9E%E8%B7%B5/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/spark java 实践/</id>
    <published>2018-10-03T02:52:20.000Z</published>
    <updated>2018-10-03T02:54:02.421Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p>SparkSession<br>SparkConf<br>SparkContext<br>JavaSparkContext<br>JavaStreamingContext<br>JavaRDD<tuple2<object, object="">&gt;<br>JavaPairRDD<tuple2<integer, integer="">, Double&gt;，有collectAsMap()方法<br>JavaRDD<tuple2<double, double="">&gt;</tuple2<double,></tuple2<integer,></tuple2<object,></p><h2 id="数据结构转换"><a href="#数据结构转换" class="headerlink" title="数据结构转换"></a>数据结构转换</h2><p>读取字符串<br>JavaRDD<string> stringItem = context.textFile(path);</string></p><p>将字符串进行拆分，map()方法返回JavaRDD数据结构<br>JavaRDD<string[]> splitedItem = stringItem.map(s -&gt; {<br>// | 为正则表达式中特殊字符，表示或操作，则需要转义<br>String[] sarray = s.split(“\|”);<br>return sarray;<br>});</string[]></p><p>将拆分结果转成key/value，mapToPair()方法返回JavaPairRDD数据结构<br>Tuple2&lt;&gt;可以自动推断数据类型<br>JavaPairRDD<float, string=""> movieRDD = splitedItem.mapToPair(line -&gt;<br>        new Tuple2&lt;&gt;(Float.parseFloat(line[0]), line[1]));</float,></p><h2 id="数据转换异常"><a href="#数据转换异常" class="headerlink" title="数据转换异常:"></a>数据转换异常:</h2><p>step 1:<br>JavaRDD<tuple2<float, string="">&gt; movieRDD = splitedItem.mapToPair(line -&gt;<br>        new Tuple2&lt;&gt;(Float.parseFloat(line[0]), line[1]));<br>step 2: 将JavaRDD通过JavaPairRDD.fromJavaRDD()方法转成JavaPairRDD，然后调用collectAsMap()，则抛出类型转换异常。</tuple2<float,></p><h2 id="ML-Pipeline"><a href="#ML-Pipeline" class="headerlink" title="ML Pipeline"></a>ML Pipeline</h2><h3 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h3><p>// 通过用户自定义函数解决数据异常值问题<br>sparkSession.udf().register(“changeDataType”, s -&gt; s.equals(“?”) ? “0” : s, DataTypes.StringType);<br>// register函数原型如下<br>public void register(String name, UDF1&lt;?, ?&gt; f, DataType returnType) {}</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据结构&quot;&gt;&lt;a href=&quot;#数据结构&quot; class=&quot;headerlink&quot; title=&quot;数据结构&quot;&gt;&lt;/a&gt;数据结构&lt;/h2&gt;&lt;p&gt;SparkSession&lt;br&gt;SparkConf&lt;br&gt;SparkContext&lt;br&gt;JavaSparkContext&lt;
      
    
    </summary>
    
      <category term="Spark" scheme="https://zlsunsuda.github.io/categories/Spark/"/>
    
    
      <category term="Java" scheme="https://zlsunsuda.github.io/tags/Java/"/>
    
      <category term="Spark" scheme="https://zlsunsuda.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法比较与选型</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E4%B8%8E%E9%80%89%E5%9E%8B/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/机器学习算法比较与选型/</id>
    <published>2018-10-03T02:52:20.000Z</published>
    <updated>2018-10-03T03:04:16.338Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>均方差 MSE mean square error<br>平均绝对差 MAE mean absolute error<br>交叉损失熵 cross entropy</p><h2 id="独热编码one-hot"><a href="#独热编码one-hot" class="headerlink" title="独热编码one-hot"></a>独热编码one-hot</h2><p>pandas，默认对所有字符串类型进行独热编码<br>one_hot_df = pd.get_dummies(data=df, columns=[‘embarked’])</p><p>keras<br>from tensorflow.python.keras.utils import np_utils<br>传入一个list，如y_train=[5 0]。<br>to_categorical将label变成one_hot。如果label为字符串，需要先通过map进行映射。<br>y_train_onehot = np_utils.to_categorical(y_train)</p><p>Tokenizer中已经封装了one_hot<br>from tensorflow.python.keras.preprocessing.text import Tokenizer<br>token = Tokenizer(num_words=2000)<br>token.fit_on_texts(X_train)<br>text_seq = token.texts_to_sequences(text)</p><p>sklearn<br>如果将一个特征进行独热编码如何做？<br>[[1], [2], [3]]为3个样本，每个样本有1个特征</p><pre><code>from sklearn.preprocessing import OneHotEncoderenc = OneHotEncoder()fit_result = enc.fit_transform([[1], [2], [3]]).toarray()print(fit_result)print(&quot;-------------------&quot;)test_onehot = enc.transform([[1]]).toarray()print(test_onehot)output: 需要对结果toarray()操作，否则结果看不懂[[1. 0. 0.][0. 1. 0.][0. 0. 1.]]-------------------[[1. 0. 0.]]</code></pre><h2 id="有监督学习算法比较"><a href="#有监督学习算法比较" class="headerlink" title="有监督学习算法比较"></a>有监督学习算法比较</h2><p>欠拟合：训练集和测试集性能接近<br>过拟合：训练集远远高于测试集性能</p><p>C, gamma越小对应越简单模型，泛化能力更强。<br>alpha越大对应越简单模型，泛化能力更强。</p><p>knn<br>调节：n_neighbors<br>优点：模型很容易解释。<br>缺点：<br>局限：仅适用于小数据集。<br>建议：预处理数据（将特征缩放到同一范围）能提高准确率。</p><p>线性模型<br>调节alpha或者C<br>优点：对维度非常高的稀疏数据（如文本数据）表现好。训练和预测速度快。适用于非常大的数据集。<br>缺点：泛化能力较差。</p><p>朴素贝叶斯模型<br>调节alpha<br>优点：对维度非常高的稀疏数据（如文本数据）表现好。训练和预测速度快。适用于非常大的数据集。<br>缺点：泛化能力比线性分类器较差。<br>局限：仅适用于分类。</p><p>决策树<br>调节 max_features, max_depth<br>优点：不需要对特征进行预处理（如放缩），不需要反复调节参数<br>缺点：容易过拟合，泛化能力差。数模型不能在训练数据范围之外进行预测。</p><p>random forest<br>调节n_estimators, max_features, max_depth<br>优点：不需要对特征进行预处理（如放缩），不需要反复调节参数。算法鲁棒性好。<br>缺点：对维度非常高的稀疏数据（如文本数据）表现不好。需要更大内存，训练和预测速度较慢。</p><p>梯度提升树<br>调节n_estimators, learning_rate。learning_rate控制每棵树对前一棵树错误的纠正强度。<br>优点：不需要对特征进行预处理（如放缩），预测时间比random forest较短，精度比random forest高。<br>缺点：对维度非常高的稀疏数据（如文本数据）表现不好。训练速度较慢。需要仔细调参，算法鲁棒性差。<br>建议：先尝试random forest，再尝试梯度提升树</p><p>svm<br>调节 核类型，gamma（等于核宽度的倒数）,C。gamma/C越大对应越复杂的模型。<br>优点：适用于低维/高维数据。决策边界可以为非线性。<br>缺点：预处理数据（需要将特征缩放到同一范围）与调参都需要非常小心（对参数设置非常敏感）。模型很难检查，很难被解释。对大数据集，运行时间较长且内存需求大。</p><p>MLP<br>调节 hidden_layer_sizes(隐含层个数，以及每个层中单元个数（一般接近于输入特征个数）)， activation（激活函数，默认为relu），solver（优化方法，默认为adam）, alpha(正则化强度)。<br>优点：在足够计算时间和数据情况下，性能更优秀。<br>缺点：预处理数据（需要将特征缩放到同一范围，均值为0， 方差为1）与调参都需要非常小心。模型很难检查，很难被解释。训练时间很长。<br>建议：adam对数据缩放相当敏感，lbfgs算法鲁棒性好。调参步骤：(1) 创建过拟合网络;(2) 缩小网络;(3)增加正则项去提高泛化能力。</p><h2 id="分类器的不确定度估计"><a href="#分类器的不确定度估计" class="headerlink" title="分类器的不确定度估计"></a>分类器的不确定度估计</h2><p>二分类<br>多分类<br>decision_function 输出为得分<br>predict_proba 输出为概率</p><h2 id="无监督学习算法比较"><a href="#无监督学习算法比较" class="headerlink" title="无监督学习算法比较"></a>无监督学习算法比较</h2><p>应用算法之前，都执行数据缩放处理。<br>kmeans<br>调节 n_clusters。<br>优点：适用于大数据集。速度较快。<br>缺点：需要给定聚类个数；k均值仅能找到相对简单的形状。依赖于初始化。<br>应用：矢量化，用簇中心来表示每个数据点。类比于pca</p><p>凝聚聚类<br>调节 n_clusters。<br>优点：适用于大数据集。速度较快。<br>缺点：需要给定聚类个数；仅能找到相对简单的形状。不能对新数据做出预测。<br>应用：找出簇中心个数</p><p>DBSCAN 带噪声的基于密度的空间聚类应用<br>调节 eps(定义了点与点之间接近的含义)，min_samples。<br>优点：适用于较大数据集。不需要给定聚类个数；能找到相对复杂的形状。<br>缺点：不能对新数据做出预测。速度较慢。<br>不确定性：有时会生成大小差别很大的簇。</p><p>###聚类算法评估<br>调整rand指数<br>归一化互信息<br>轮廓系数（在实践中效果不佳）<br>基于鲁棒性的聚类指标（sklearn暂未实现）</p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>(1) fit, transform用fit_transform代替<br>对训练数据与测试数据执行相同的尺度变化操作<br>(2) 数据偏斜问题（分布不均匀）</p><p>pca<br>调节 n_components, whiten (pca白化，将主成分缩放到相同尺度，包括缩放数据和旋转数据)<br>应用：降维。寻找数据中方差最大的方向。<br>缺点：不容易解释降维结果</p><p>应用：特征提取（如提取人脸特征）</p><p>nmf （非负矩阵分解）<br>不对数据进行重建或编码，在数据中寻找有趣模式。比PCA获取的成分更容易解释。</p><p>流行学习算法<br>t-SNE，用于数据可视化</p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>一般特征满足高斯分布时模型会获取到更好性能<br>特征分箱，将连续特征拆分为多个离散特征，一般对单一特征做分箱，提供线性模型性能。<br>one-hot编码</p><p>特征提取</p><p>特征选择<br>去除噪声特征便于构建模型。特征选择与PCA区别？<br>SelectKBest<br>SelectPercentile<br>SelectFromModel<br>RFE，recursive feature elimination，递归特征删除</p><p>多项式特征(x<em>x, x</em>x<em>x), preprocessing.PolynomialFeatures<br>交互项特征 (x1 </em> x2)</p><h2 id="模型验证"><a href="#模型验证" class="headerlink" title="模型验证"></a>模型验证</h2><p>评价泛化能力<br>train_test_split</p><p>k折交叉验证<br>调节 cv，用于指定分类器<br>cross_val_score<br>优点：比更train_test_split准确。<br>缺点：增加了计算成本。<br>应用：用于回归</p><p>分层k折交叉验证<br>避免某一个类别的数据都集中在某一个折中。导入sklearn.model_selection.StratifiedKFold<br>优点：比更train_test_split准确。<br>缺点：增加了计算成本。<br>应用：用于分类</p><p>打乱交叉验证<br>sklearn.model_selection.ShuffleSplit</p><p>分组交叉验证<br>比如将同一个人的数据放在一组<br>sklearn.model_selection.GroupKFold</p><h2 id="模型调参"><a href="#模型调参" class="headerlink" title="模型调参"></a>模型调参</h2><p>样本分成3部分：训练集，验证集，测试集<br>带交叉验证的网格搜索, sklearn.model_selection.GridSearchCV<br>GridSearchCV不仅能搜索最佳参数，而且能拟合出一个性能更优秀的模型</p><h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><h3 id="评估分类模型"><a href="#评估分类模型" class="headerlink" title="评估分类模型"></a>评估分类模型</h3><p>准确率<br>召回率<br>f1分数</p><p>混淆矩阵<br>对二分类问题最全面的表示，输出对整个样本预测结果。主对角线上元素对应正确的分类。</p><p>sklearn.metrics.confusion_matrix<br>输出为一个2*2矩阵，分别对应预测为TN(true negtive), FP, FN, TP（true positive）个数。</p><p>ROC receiver operating characteristics curve， 受试者工作特征曲线<br>AUC area under curve，为ROC曲线下面积<br>sklearn.metrics.roc_curve<br>sklearn.metrics.roc_auc_score<br>对分布不平衡数据进行分类，推荐用AUC。取值[0,1],越大值说明性能更好。</p><p>准确率-召回率曲线<br>sklearn.metrics.average_precision_score</p><h3 id="评估回归模型"><a href="#评估回归模型" class="headerlink" title="评估回归模型"></a>评估回归模型</h3><p>sklearn.metrics.r2_score<br>sklearn.metrics.mean_squared_error<br>sklearn.metrics.mean_absolute_error</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h2&gt;&lt;p&gt;均方差 MSE mean square error&lt;br&gt;平均绝对差 MAE mean absolute error&lt;br&gt;交叉损失熵 cr
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://zlsunsuda.github.io/categories/Machine-Learning/"/>
    
    
      <category term="SKlearn" scheme="https://zlsunsuda.github.io/tags/SKlearn/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB慢查询分析</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/MongoDB%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/MongoDB慢查询分析/</id>
    <published>2018-10-03T00:42:20.000Z</published>
    <updated>2018-10-03T02:13:48.521Z</updated>
    
    <content type="html"><![CDATA[<h2 id="定义MongoDB慢查询"><a href="#定义MongoDB慢查询" class="headerlink" title="定义MongoDB慢查询"></a>定义MongoDB慢查询</h2><p>db.setProfilingLevel(1)<br>db.setProfilingLevel(1,1000)<br>db.getProfilingStatus()</p><h2 id="查看记录下来的耗时语句"><a href="#查看记录下来的耗时语句" class="headerlink" title="查看记录下来的耗时语句"></a>查看记录下来的耗时语句</h2><p>db.system.profile.find()<br>其中，”ts” : ISODate(“2018-03-28T01:53:10.779Z”)为执行命令时间：<br>db.system.profile.find().sort({ts:-1})</p><h2 id="通过explain-查看index使用情况"><a href="#通过explain-查看index使用情况" class="headerlink" title="通过explain()查看index使用情况"></a>通过explain()查看index使用情况</h2><p>db.getCollection(‘collection_name’).find({“insertTime”:{“$gte”:ISODate(“2018-02-06T05:47:00.267Z”), “$lte”:ISODate(“2018-03-06T05:47:00.267Z”)}}).explain()</p><p>删除histroy profile，先后执行如下两条命令：<br>db.setProfilingLevel(0)<br>db.system.profile.drop()<br>从浏览器查看后台响应时间</p><h2 id="索引使用情况"><a href="#索引使用情况" class="headerlink" title="索引使用情况"></a>索引使用情况</h2><p>“planSummary” : “IXSCAN { factoryId: 1, insertTime: -1, zone: 1, name: 1, eventId: 1 }”,</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;定义MongoDB慢查询&quot;&gt;&lt;a href=&quot;#定义MongoDB慢查询&quot; class=&quot;headerlink&quot; title=&quot;定义MongoDB慢查询&quot;&gt;&lt;/a&gt;定义MongoDB慢查询&lt;/h2&gt;&lt;p&gt;db.setProfilingLevel(1)&lt;br&gt;db.
      
    
    </summary>
    
      <category term="MongoDB" scheme="https://zlsunsuda.github.io/categories/MongoDB/"/>
    
    
      <category term="MongoDB" scheme="https://zlsunsuda.github.io/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>Redis issue</title>
    <link href="https://zlsunsuda.github.io/2018/10/03/Redis%20issuemd/"/>
    <id>https://zlsunsuda.github.io/2018/10/03/Redis issuemd/</id>
    <published>2018-10-03T00:42:20.000Z</published>
    <updated>2018-10-03T03:05:29.672Z</updated>
    
    <content type="html"><![CDATA[<h2 id="redis性能排查步骤"><a href="#redis性能排查步骤" class="headerlink" title="redis性能排查步骤"></a>redis性能排查步骤</h2><p>redis-cli命令<br>cd /usr/local/redis-stable/bin/<br>登录：./redis-cli -h 10.11.60.8 -p 6379 -a foobared<br>在交互方式下查看信息：<br>查看连接池<br>info clients</p><h2 id="redis-exception"><a href="#redis-exception" class="headerlink" title="redis exception"></a>redis exception</h2><h2 id="连接存在异常"><a href="#连接存在异常" class="headerlink" title="连接存在异常"></a>连接存在异常</h2><p>org.springframework.data.redis.RedisConnectionFailureException: Cannot get Jedis connection</p><h3 id="批量操作"><a href="#批量操作" class="headerlink" title="批量操作"></a>批量操作</h3><p>Callback cannot return a non-null value as it gets overwritten by the pipeline</p><h3 id="redisson-连接异常"><a href="#redisson-连接异常" class="headerlink" title="redisson 连接异常"></a>redisson 连接异常</h3><p>org.redisson.client.RedisConnectionException: Can’t init enough connections amount! Only 8 from 10 were initialized. Server: </p><p>org.springframework.data.redis.RedisConnectionFailureException: java.net.SocketTimeoutException: Read timed out; nested exception is redis.clients.jedis.exceptions.JedisConnectionException: java.net.SocketTimeoutException: Read timed out</p><p>可以在安装redis服务机器中查看已经建立的连接个数：<br>netstat -antl | grep 6379<br>或者<br>netstat -antl | grep 6379 | wc -l</p><p>查看已经创建的连接，目的端口固定，而源端口随机</p><h3 id="redisson版本问题"><a href="#redisson版本问题" class="headerlink" title="redisson版本问题"></a>redisson版本问题</h3><p>对3.5.x版本<br>Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 0: 10.11.42.151:6379</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;redis性能排查步骤&quot;&gt;&lt;a href=&quot;#redis性能排查步骤&quot; class=&quot;headerlink&quot; title=&quot;redis性能排查步骤&quot;&gt;&lt;/a&gt;redis性能排查步骤&lt;/h2&gt;&lt;p&gt;redis-cli命令&lt;br&gt;cd /usr/local/redis
      
    
    </summary>
    
      <category term="Redis" scheme="https://zlsunsuda.github.io/categories/Redis/"/>
    
    
      <category term="Redis" scheme="https://zlsunsuda.github.io/tags/Redis/"/>
    
  </entry>
  
</feed>
